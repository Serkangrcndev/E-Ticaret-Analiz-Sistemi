# Site GÃ¼venlik Analizi Projesi

Bu proje, web sitelerinin gÃ¼venlik ve itibar analizini yapmak iÃ§in geliÅŸtirilmiÅŸ bir web scraping ve analiz platformudur. FarklÄ± kaynaklardan (Åikayetvar, Trustpilot, Google Reviews) toplanan verileri analiz ederek sitelerin risk skorlarÄ±nÄ± hesaplar ve kullanÄ±cÄ± dostu bir arayÃ¼zle sunar.

## ğŸ¯ Ã–zellikler

- **Ã‡oklu Kaynak DesteÄŸi**: Åikayetvar, Trustpilot ve Google Reviews'den otomatik veri toplama
- **Risk Analizi**: Toplanan verilere gÃ¶re otomatik risk skoru hesaplama
- **Sentiment Analizi**: Åikayet ve yorumlarÄ±n duygu analizi (pozitif/negatif/nÃ¶tr)
- **Modern Web ArayÃ¼zÃ¼**: React tabanlÄ± responsive ve kullanÄ±cÄ± dostu frontend
- **RESTful API**: Flask tabanlÄ± gÃ¼Ã§lÃ¼ backend API
- **VeritabanÄ± YÃ¶netimi**: SQL Server ile gÃ¼venli veri saklama
- **Ã‡Ã¶zÃ¼mlenmiÅŸ/Ã‡Ã¶zÃ¼lmemiÅŸ Takibi**: Åikayetlerin Ã§Ã¶zÃ¼m durumunu takip etme

## ğŸ› ï¸ Teknolojiler

### Backend
- **Python 3.x**
- **Flask** - Web framework
- **SQL Server** - VeritabanÄ±
- **Selenium** - Web scraping
- **BeautifulSoup4** - HTML parsing
- **pyodbc** - SQL Server baÄŸlantÄ±sÄ±

### Frontend
- **React 18** - UI framework
- **Vite** - Build tool
- **React Router** - Routing
- **Axios** - HTTP client

## ğŸ“‹ Gereksinimler

### Backend
- Python 3.8+
- SQL Server (Express veya Ã¼zeri)
- ODBC Driver 18 for SQL Server
- Chrome/Chromium (Selenium iÃ§in)

### Frontend
- Node.js 16+
- npm veya yarn

## ğŸš€ Kurulum

### 1. Backend Kurulumu

```bash
# Proje dizinine gidin
cd backend

# Sanal ortam oluÅŸturun (Ã¶nerilir)
python -m venv venv

# Sanal ortamÄ± aktifleÅŸtirin
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin
pip install -r requirements.txt
```

### 2. VeritabanÄ± YapÄ±landÄ±rmasÄ±

1. SQL Server'Ä± baÅŸlatÄ±n
2. `.env` dosyasÄ± oluÅŸturun veya `config.py` dosyasÄ±nÄ± dÃ¼zenleyin:

```env
SQL_SERVER=localhost\SQLEXPRESS
SQL_DATABASE=SiteGuvenlikDB
SQL_USERNAME=sa
SQL_PASSWORD=your_password
SQL_DRIVER=ODBC Driver 18 for SQL Server
API_HOST=0.0.0.0
API_PORT=5000
DEBUG=False
```

3. VeritabanÄ± tablolarÄ±nÄ± oluÅŸturun:
   - API'yi baÅŸlattÄ±ktan sonra `/api/init-db` endpoint'ine POST isteÄŸi gÃ¶nderin
   - Veya `create_tables.sql` dosyasÄ±nÄ± SQL Server'da Ã§alÄ±ÅŸtÄ±rÄ±n

### 3. Frontend Kurulumu

```bash
# Frontend dizinine gidin
cd frontend

# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin
npm install
# veya
yarn install
```

### 4. API YapÄ±landÄ±rmasÄ±

Frontend'in backend'e baÄŸlanabilmesi iÃ§in `frontend/src/services/api.js` dosyasÄ±ndaki API URL'ini kontrol edin.

## ğŸ® KullanÄ±m

### Backend'i BaÅŸlatma

```bash
cd backend
python app.py
```

Veya Windows'ta:
```bash
start_backend.bat
```

Backend varsayÄ±lan olarak `http://localhost:5000` adresinde Ã§alÄ±ÅŸacaktÄ±r.

### Frontend'i BaÅŸlatma

```bash
cd frontend
npm run dev
# veya
yarn dev
```

Veya Windows'ta:
```bash
start_frontend.bat
```

Frontend varsayÄ±lan olarak `http://localhost:5173` adresinde Ã§alÄ±ÅŸacaktÄ±r.

### Site Analizi Yapma

1. Frontend arayÃ¼zÃ¼nde ana sayfaya gidin
2. Analiz etmek istediÄŸiniz sitenin URL'sini girin
3. "Analiz Et" butonuna tÄ±klayÄ±n
4. Analiz tamamlandÄ±ÄŸÄ±nda sonuÃ§larÄ± gÃ¶rÃ¼ntÃ¼leyebilirsiniz

## ğŸ“¡ API Endpoints

### Genel
- `GET /` - API bilgileri ve endpoint listesi
- `GET /api/health` - API saÄŸlÄ±k kontrolÃ¼
- `GET /api/db-status` - VeritabanÄ± baÄŸlantÄ± durumu

### Site Ä°ÅŸlemleri
- `POST /api/analyze` - Site analizi baÅŸlat
  ```json
  {
    "url": "https://example.com"
  }
  ```
- `GET /api/sites` - TÃ¼m siteleri listele
- `GET /api/site/<domain>` - Belirli bir site bilgilerini getir

### VeritabanÄ±
- `POST /api/init-db` - VeritabanÄ± tablolarÄ±nÄ± oluÅŸtur
- `POST /api/migrate-isresolved` - IsResolved sÃ¼tunu migration

## ğŸ—„ï¸ VeritabanÄ± YapÄ±sÄ±

### Sites Tablosu
- `SiteID` (Primary Key)
- `Domain` (Unique)
- `SiteName`
- `RiskScore` (0-100)
- `LastScannedDate`
- `CreatedDate`

### Complaints Tablosu
- `ComplaintID` (Primary Key)
- `SiteID` (Foreign Key)
- `Source` (sikayetvar, trustpilot, google_reviews)
- `Title`
- `Content`
- `Author`
- `Date`
- `Rating`
- `Sentiment` (positive, negative, neutral)
- `URL`
- `IsResolved` (boolean)

## ğŸ“ Proje YapÄ±sÄ±

```
Web Scrapper new/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                 # Flask uygulamasÄ±
â”‚   â”œâ”€â”€ config.py              # YapÄ±landÄ±rma
â”‚   â”œâ”€â”€ database.py            # VeritabanÄ± iÅŸlemleri
â”‚   â”œâ”€â”€ scraper_service.py     # Scraping servisi
â”‚   â”œâ”€â”€ requirements.txt       # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”‚   â”œâ”€â”€ create_tables.sql      # SQL tablo oluÅŸturma scripti
â”‚   â””â”€â”€ scrapers/
â”‚       â”œâ”€â”€ base_scraper.py    # Temel scraper sÄ±nÄ±fÄ±
â”‚       â”œâ”€â”€ sikayetvar_scraper.py
â”‚       â”œâ”€â”€ trustpilot_scraper.py
â”‚       â””â”€â”€ google_reviews_scraper.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx            # Ana React bileÅŸeni
â”‚   â”‚   â”œâ”€â”€ components/        # UI bileÅŸenleri
â”‚   â”‚   â”œâ”€â”€ pages/             # Sayfa bileÅŸenleri
â”‚   â”‚   â”œâ”€â”€ services/          # API servisleri
â”‚   â”‚   â””â”€â”€ utils/             # YardÄ±mcÄ± fonksiyonlar
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â””â”€â”€ README.md
```

## âš™ï¸ YapÄ±landÄ±rma

### Backend YapÄ±landÄ±rmasÄ±

`backend/config.py` veya `.env` dosyasÄ± Ã¼zerinden yapÄ±landÄ±rma yapÄ±labilir:

- `SQL_SERVER`: SQL Server adresi ve instance
- `SQL_DATABASE`: VeritabanÄ± adÄ±
- `SQL_USERNAME`: VeritabanÄ± kullanÄ±cÄ± adÄ±
- `SQL_PASSWORD`: VeritabanÄ± ÅŸifresi
- `API_HOST`: API host adresi (0.0.0.0 = tÃ¼m interface'ler)
- `API_PORT`: API port numarasÄ±
- `SCRAPING_DELAY`: Ä°stekler arasÄ± bekleme sÃ¼resi (saniye)
- `MAX_RESULTS`: Maksimum sonuÃ§ sayÄ±sÄ±

### Frontend YapÄ±landÄ±rmasÄ±

`frontend/src/services/api.js` dosyasÄ±nda API base URL'i ayarlanabilir.

## ğŸ”§ GeliÅŸtirme

### Yeni Scraper Ekleme

1. `backend/scrapers/base_scraper.py` sÄ±nÄ±fÄ±ndan tÃ¼retin
2. `scrape()` metodunu implement edin
3. `scraper_service.py` iÃ§inde yeni scraper'Ä± kaydedin

### VeritabanÄ± Migration

Yeni sÃ¼tun veya tablo eklemek iÃ§in:
1. `database.py` iÃ§inde migration metodlarÄ± ekleyin
2. `app.py` iÃ§inde migration endpoint'i oluÅŸturun

## âš ï¸ Ã–nemli Notlar

- **Rate Limiting**: Web scraping yaparken kaynak sitelerin rate limit'lerine dikkat edin
- **Legal**: Scraping yapmadan Ã¶nce hedef sitelerin kullanÄ±m ÅŸartlarÄ±nÄ± kontrol edin
- **Production**: Production ortamÄ±nda Waitress veya Gunicorn gibi WSGI server'larÄ± kullanÄ±n
- **GÃ¼venlik**: `.env` dosyasÄ±nÄ± `.gitignore`'a ekleyin ve hassas bilgileri commit etmeyin
- **VeritabanÄ±**: SQL Server baÄŸlantÄ± pool'u kullanÄ±larak performans optimize edilmiÅŸtir

## ğŸ› Sorun Giderme

### VeritabanÄ± BaÄŸlantÄ± HatasÄ±
- SQL Server'Ä±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun
- ODBC Driver 18'in yÃ¼klÃ¼ olduÄŸunu kontrol edin
- BaÄŸlantÄ± bilgilerini (`config.py` veya `.env`) kontrol edin
- Firewall ayarlarÄ±nÄ± kontrol edin

### Scraping HatasÄ±
- Chrome/Chromium'un yÃ¼klÃ¼ olduÄŸundan emin olun
- Selenium WebDriver'Ä±n gÃ¼ncel olduÄŸunu kontrol edin
- Ä°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin
- Hedef sitenin eriÅŸilebilir olduÄŸunu kontrol edin

### Frontend BaÄŸlantÄ± HatasÄ±
- Backend'in Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun
- CORS ayarlarÄ±nÄ± kontrol edin
- API URL'ini kontrol edin

## ğŸ“ Lisans

Bu proje Ã¶zel bir projedir.

## ğŸ‘¤ GeliÅŸtirici

Serkan GÃ¼rcan

---

**Not**: Bu proje geliÅŸtirme aÅŸamasÄ±ndadÄ±r. Production kullanÄ±mÄ± iÃ§in ek gÃ¼venlik ve optimizasyon Ã¶nlemleri alÄ±nmalÄ±dÄ±r.

